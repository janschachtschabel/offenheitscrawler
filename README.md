# ğŸ•·ï¸ Offenheitscrawler

[![Python](https://img.shields.io/badge/Python-3.8%2B-blue.svg)](https://python.org)
[![Streamlit](https://img.shields.io/badge/Streamlit-1.28%2B-red.svg)](https://streamlit.io)
[![License: Apache 2.0](https://img.shields.io/badge/License-Apache%202.0-blue.svg)](https://opensource.org/licenses/Apache-2.0)

Ein intelligenter, KI-gestÃ¼tzter Web-Crawler mit modernem Streamlit-Interface zur automatischen Bewertung von Offenheitskriterien bei verschiedenen Organisationstypen.

[![Open in Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/drive/1Mnpwi9xlfdoUsNpBvwRCHkkiLmUkY2s3?usp=sharing)

**ğŸ“Š [Demo: Offenheitscrawler Google Colab Notebook](https://colab.research.google.com/drive/1Mnpwi9xlfdoUsNpBvwRCHkkiLmUkY2s3?usp=sharing)**

> Testen Sie den Offenheitscrawler direkt im Browser â€“ keine lokale Installation nÃ¶tig!

## âœ¨ Hauptfunktionen

### ğŸ¤– KI-gestÃ¼tzte Analyse
- **Large Language Model Integration**: Nutzt OpenAI GPT fÃ¼r intelligente Kriterienbewertung
- **Automatische Textanalyse**: Erkennt und bewertet Offenheitskriterien in Webinhalten
- **Konfidenz-Scoring**: Jede Bewertung wird mit einem Vertrauenswert versehen
- **Kontextuelle BegrÃ¼ndungen**: Detaillierte ErklÃ¤rungen fÃ¼r jede Bewertung

### ğŸ•¸ï¸ Erweiterte Crawling-Funktionen
- **Crawl4AI Integration**: Moderne, asynchrone Crawling-Engine
- **Intelligente Navigation**: Automatische Erkennung relevanter Unterseiten
- **Robuste Fehlerbehandlung**: Graceful Handling von Netzwerkfehlern und Timeouts
- **Flexible Tiefensteuerung**: Konfigurierbare Crawling-Tiefe pro Organisation

### ğŸ“Š Umfassende Visualisierungen
- **Automatische Diagrammerstellung**: PNG-Export mit Matplotlib und Seaborn
- **Interaktive Dashboards**: Streamlit-basierte Echtzeit-Visualisierungen
- **Mehrdimensionale Statistiken**: Organisations- und kriterienbasierte Auswertungen
- **Export-Funktionen**: Download von Grafiken und Daten in verschiedenen Formaten

### ğŸ¯ Multi-Format Export
- **Drei CSV-Ausgabeformate**: Detaillierte Kriterien, Organisations-Statistiken, Kriterien-Statistiken
- **Automatische Visualisierungen**: Balkendiagramme, Histogramme, Scatterplots, Tortendiagramme
- **Zeitstempel-basierte Archivierung**: Eindeutige Dateinamen fÃ¼r historische Nachverfolgung
- **Excel-kompatible Formate**: UTF-8-SIG Encoding fÃ¼r problemlose Excel-Integration

## ğŸ—ï¸ UnterstÃ¼tzte Organisationstypen

Jeder Organisationstyp verfÃ¼gt Ã¼ber einen spezialisierten YAML-Kriterienkatalog mit hierarchischer Struktur:

| Typ | Beschreibung | Kriterien-Dimensionen |
|-----|-------------|----------------------|
| ğŸ“ **Bildungseinrichtungen** | Schulen, Bildungsinstitute | Transparenz, Partizipation, Rechenschaftslegung |
| ğŸ›ï¸ **Hochschulen** | UniversitÃ¤ten, Fachhochschulen | Governance, Forschung, Lehre, Verwaltung |
| ğŸ­ **Kultureinrichtungen** | Museen, Theater, Bibliotheken | Kulturelle Offenheit, ZugÃ¤nglichkeit, Partizipation |
| ğŸ”¬ **Forschungseinrichtungen** | Institute, Forschungszentren | Open Science, Datenoffenheit, Kollaboration |
| ğŸ“º **Ã–ffentlich-rechtlicher Rundfunk** | TV- und Radiosender | Medientransparenz, Programmoffenheit, BÃ¼rgernÃ¤he |

### ğŸ“‹ Kriterien-Hierarchie

Jeder Katalog folgt einer einheitlichen 4-stufigen Struktur:
```
Dimensionen â†’ Faktoren â†’ Faktortypen â†’ Kriterien
```

## ğŸš€ Installation

### Voraussetzungen
- Python 3.8 oder hÃ¶her
- OpenAI API-SchlÃ¼ssel (fÃ¼r KI-gestÃ¼tzte Bewertungen)
- Mindestens 4GB RAM (fÃ¼r grÃ¶ÃŸere Crawling-Operationen)

### Schritt-fÃ¼r-Schritt Installation

```bash
# 1. Repository klonen
git clone https://github.com/janschachtschabel/offenheitscrawler
cd offenheitscrawler

# 2. Virtuelle Umgebung erstellen
python -m venv venv

# 3. Umgebung aktivieren
# Linux/Mac:
source venv/bin/activate
# Windows:
venv\Scripts\activate

# 4. Dependencies installieren
pip install -r requirements.txt

# 5. Umgebungsvariablen konfigurieren (optional)
cp .env.example .env
# Bearbeiten Sie .env mit Ihrem OpenAI API-SchlÃ¼ssel
```

### ğŸ“¦ HauptabhÃ¤ngigkeiten

- **Streamlit** (â‰¥1.28.0) - Web-Interface
- **Crawl4AI** (â‰¥0.2.0) - Moderne Crawling-Engine
- **OpenAI** (â‰¥1.0.0) - KI-Integration
- **Pandas** (â‰¥2.0.0) - Datenverarbeitung
- **Matplotlib** (â‰¥3.7.0) - Visualisierungen
- **Seaborn** (â‰¥0.12.0) - Erweiterte Statistik-Plots
- **BeautifulSoup4** (â‰¥4.12.0) - HTML-Parsing
- **Loguru** (â‰¥0.7.0) - Erweiterte Protokollierung

## ğŸ’» Verwendung

### ğŸš€ Anwendung starten

```bash
# Streamlit-App starten
streamlit run app.py
```

Die App Ã¶ffnet sich automatisch im Browser unter `http://localhost:8501`.

### ğŸ“‹ Detaillierter Workflow

#### 1ï¸âƒ£ **Vorbereitung**
- ğŸ“ CSV-Datei mit Organisationen vorbereiten (Format: `Name;URL`)
- ğŸ”‘ OpenAI API-SchlÃ¼ssel in den Einstellungen konfigurieren
- âš™ï¸ Crawling-Parameter nach Bedarf anpassen

#### 2ï¸âƒ£ **Datenimport**
- ğŸ“„ CSV-Datei hochladen (automatische Format-Erkennung)
- ğŸ¯ Organisationsbereich auswÃ¤hlen (alle, Bereich, einzelne)
- ğŸ“‹ Organisationstyp und Kriterienkatalog wÃ¤hlen

#### 3ï¸âƒ£ **Konfiguration**
- ğŸ” **Crawling-Tiefe**: Anzahl der zu durchsuchenden Unterseiten
- ğŸ”„ **Retry-Versuche**: Anzahl Wiederholungen bei Fehlern
- ğŸ¯ **Konfidenz-Schwellwert**: Mindestvertrauen fÃ¼r Bewertungen
- ğŸš€ **Crawling-Strategie**: Standard oder erweitert

#### 4ï¸âƒ£ **Crawling-Prozess**
- â–¶ï¸ Crawling starten und Echtzeit-Fortschritt verfolgen
- ğŸ“Š Live-Statistiken zu erfolgreichen/fehlgeschlagenen Organisationen
- â¸ï¸ MÃ¶glichkeit zum Pausieren/Fortsetzen
- ğŸ“ Detaillierte Logs fÃ¼r Debugging

#### 5ï¸âƒ£ **Ergebnisanalyse**
- ğŸ“Š **Statistiken-Dashboard**: Ãœbersicht Ã¼ber alle Ergebnisse
- ğŸ—ºï¸ **Interaktive Visualisierungen**: Automatisch generierte Diagramme
- ğŸ” **Detailansicht**: Einzelne Organisationen und Kriterien erkunden
- ğŸ“ˆ **Performance-Metriken**: ErfÃ¼llungsraten und Konfidenzwerte

#### 6ï¸âƒ£ **Export & Reporting**
- ğŸ“ **Drei CSV-Formate**: Detailliert, Organisations-Stats, Kriterien-Stats
- ğŸ–¼ï¸ **Automatische Visualisierungen**: PNG-Grafiken mit hoher AuflÃ¶sung
- ğŸ“… **Zeitstempel-Archivierung**: Historische Nachverfolgung
- ğŸ“Š **JSON-Export**: Maschinenlesbare Zusammenfassungen

### ğŸŒ Hauptbereiche der Anwendung

| Bereich | Funktion | Beschreibung |
|---------|----------|-------------|
| ğŸ  **Hauptseite** | Crawling & Konfiguration | CSV-Upload, Einstellungen, Crawling-Start |
| ğŸ“Š **Statistiken** | Analyse & Visualisierung | Dashboards, Diagramme, Detailansichten |
| âš™ï¸ **Einstellungen** | Konfiguration | API-Keys, Crawling-Parameter, Erweiterte Optionen |

## ğŸ“ Projektstruktur

```
offenheitscrawler/
â”œâ”€â”€ app.py                     # ğŸ  Streamlit Hauptanwendung
â”œâ”€â”€ requirements.txt           # ğŸ“¦ Python-AbhÃ¤ngigkeiten
â”œâ”€â”€ README.md                  # ğŸ“ Dokumentation
â”œâ”€â”€ src/                       # ğŸ’» Hauptquellcode
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ crawler/               # ğŸ•·ï¸ Crawling-Engine
â”‚   â”‚   â”œâ”€â”€ crawl4ai_crawler.py    # Moderne Crawling-Implementierung
â”‚   â”‚   â”œâ”€â”€ organization_crawler.py # Organisations-spezifisches Crawling
â”‚   â”‚   â””â”€â”€ page_analyzer.py       # Seitenanalyse und -bewertung
â”‚   â”œâ”€â”€ analysis/              # ğŸ¤– KI-gestÃ¼tzte Analyse
â”‚   â”‚   â”œâ”€â”€ llm_evaluator.py       # OpenAI GPT Integration
â”‚   â”‚   â”œâ”€â”€ criteria_analyzer.py   # Kriterienauswertung
â”‚   â”‚   â””â”€â”€ confidence_scorer.py   # Konfidenz-Bewertung
â”‚   â”œâ”€â”€ config/                # âš™ï¸ Konfiguration & YAML-Loader
â”‚   â”‚   â”œâ”€â”€ yaml_loader.py         # YAML-Katalog-Parser
â”‚   â”‚   â””â”€â”€ settings_manager.py    # Einstellungsverwaltung
â”‚   â”œâ”€â”€ statistics/            # ğŸ“Š Statistiken & Reporting
â”‚   â”‚   â”œâ”€â”€ visualizations.py      # Matplotlib/Seaborn Plots
â”‚   â”‚   â””â”€â”€ report_generator.py    # CSV/JSON Export
â”‚   â”œâ”€â”€ ui/                    # ğŸ“± Streamlit UI-Komponenten
â”‚   â”‚   â”œâ”€â”€ main_page.py           # Haupt-Crawling-Interface
â”‚   â”‚   â”œâ”€â”€ statistics_page.py     # Statistiken & Visualisierungen
â”‚   â”‚   â””â”€â”€ settings_page.py       # Konfigurationsseite
â”‚   â””â”€â”€ utils/                 # ğŸ”§ Hilfsfunktionen
â”‚       â”œâ”€â”€ csv_handler.py         # CSV Import/Export
â”‚       â”œâ”€â”€ logger.py              # Erweiterte Protokollierung
â”‚       â””â”€â”€ validators.py          # Datenvalidierung
â”œâ”€â”€ criteria/                  # ğŸ“‹ YAML-Kriterienkataloge
â”‚   â”œâ”€â”€ bildungseinrichtungen.yaml
â”‚   â”œâ”€â”€ hochschulen.yaml
â”‚   â”œâ”€â”€ kultureinrichtungen.yaml
â”‚   â”œâ”€â”€ forschungseinrichtungen.yaml
â”‚   â””â”€â”€ oeffentlich_rechtlicher_rundfunk.yaml
â”œâ”€â”€ output/                    # ğŸ“ Generierte Ausgaben
â”‚   â”œâ”€â”€ *.csv                  # CSV-Exporte mit Zeitstempel
â”‚   â””â”€â”€ visualizations/        # Automatisch generierte Grafiken
â”œâ”€â”€ tests/                     # ğŸ§ª Unit Tests
â”œâ”€â”€ data/                      # ğŸ“Š Beispieldaten
â””â”€â”€ docs/                      # ğŸ“š Erweiterte Dokumentation
```

## ğŸ”§ Entwicklung

### ğŸ“ Code-QualitÃ¤t

```bash
# Code-QualitÃ¤t prÃ¼fen
ruff check .
ruff format .

# Type-Checking
mypy src/

# Tests ausfÃ¼hren
pytest

# Coverage-Report
pytest --cov=src --cov-report=html
```

### ğŸ” Debugging

```bash
# Detaillierte Logs aktivieren
streamlit run app.py --logger.level=DEBUG

# Einzelne Module testen
python -m src.crawler.crawl4ai_crawler
python -m src.analysis.llm_evaluator
```

### ğŸ“š API-Dokumentation

```bash
# Automatische Dokumentation generieren
sphinx-build -b html docs/ docs/_build/
```

## ğŸ“ CSV-Format (Input)

Die CSV-Datei unterstÃ¼tzt flexible Formate mit automatischer Erkennung:

### âœ… UnterstÃ¼tzte Formate

**Standard-Format mit Kopfzeile:**
```csv
Organisation;URL
UniversitÃ¤t MÃ¼nchen;https://www.lmu.de
TU Berlin;https://www.tu-berlin.de
ETH ZÃ¼rich;https://ethz.ch
```

**Ohne Kopfzeile:**
```csv
UniversitÃ¤t MÃ¼nchen;https://www.lmu.de
TU Berlin;https://www.tu-berlin.de
ETH ZÃ¼rich;https://ethz.ch
```

**Erweiterte Spalten (werden ignoriert):**
```csv
Organisation;URL;Stadt;Land;Typ
UniversitÃ¤t MÃ¼nchen;https://www.lmu.de;MÃ¼nchen;Deutschland;UniversitÃ¤t
```

### ğŸ“ Format-Spezifikationen

| Parameter | Wert | Beschreibung |
|-----------|------|-------------|
| **Trennzeichen** | `;` (Semikolon) | Standard-Separator |
| **Encoding** | UTF-8 | UnterstÃ¼tzt Umlaute und Sonderzeichen |
| **Kopfzeile** | Optional | Automatische Erkennung |
| **Minimale Spalten** | 2 | Name und URL erforderlich |
| **Maximale Zeilen** | Unbegrenzt | AbhÃ¤ngig von Systemressourcen |

## ğŸ“„ Output-Formate

Die Anwendung generiert umfassende Berichte in verschiedenen Formaten:

### ğŸ“ CSV-Exporte (3 Dateien)

#### 1. **Detaillierte Kriterienbewertungen**
```csv
Organisation;Basis_URL;Kriterium_ID;Kriterium_Name;ErfÃ¼llt;Konfidenz;BegrÃ¼ndung;Quelle_URL;Beweis_Text;Muster_Typ
UniversitÃ¤t XYZ;https://uni-xyz.de;transparenz_finanzen;Transparenz Finanzen;Ja;0.85;Jahresbericht gefunden;https://uni-xyz.de/finanzen;Der Jahresbericht 2023 zeigt...;financial_report
```

#### 2. **Organisations-Statistiken**
```csv
Organisation;Basis_URL;Anzahl_Kriterien;ErfÃ¼llte_Kriterien;ErfÃ¼llungsgrad;Durchschnittliche_Konfidenz;Erfolgreiche_Seiten;Fehlgeschlagene_Seiten
UniversitÃ¤t XYZ;https://uni-xyz.de;25;18;72.0;0.78;12;3
```

#### 3. **Kriterien-Statistiken**
```csv
Kriterium_ID;Kriterium_Name;Anzahl_Organisationen;ErfÃ¼llte_Organisationen;ErfÃ¼llungsrate;Durchschnittliche_Konfidenz
transparenz_finanzen;Transparenz Finanzen;50;35;70.0;0.82
```

### ğŸ–¼ï¸ Automatische Visualisierungen

Jede Crawling-Session generiert automatisch hochauflÃ¶sende PNG-Grafiken:

| Visualisierung | Dateiname | Beschreibung |
|----------------|-----------|-------------|
| **Organisations-Ãœbersicht** | `organisationen_uebersicht_[katalog]_[timestamp].png` | Balkendiagramm und Histogramm der ErfÃ¼llungsgrade |
| **Kriterien-Performance** | `kriterien_performance_[katalog]_[timestamp].png` | Scatterplot Konfidenz vs. ErfÃ¼llungsrate |
| **Zusammenfassende Statistik** | `zusammenfassung_statistik_[katalog]_[timestamp].png` | Top/Bottom Kriterien, Tortendiagramme, Boxplots |

### ğŸ“Š JSON-Export

```json
{
  "timestamp": "2024-01-15T14:30:22",
  "catalog_used": "hochschulen",
  "total_organizations": 50,
  "successful_organizations": 45,
  "overall_fulfillment_rate": 68.5,
  "organizations": [
    {
      "name": "UniversitÃ¤t XYZ",
      "url": "https://uni-xyz.de",
      "fulfillment_rate": 72.0,
      "total_criteria": 25,
      "fulfilled_criteria": 18
    }
  ]
}
```

## âš™ï¸ Erweiterte Konfiguration

### ğŸ”‘ OpenAI API-Konfiguration

```bash
# Umgebungsvariable setzen
export OPENAI_API_KEY="your-api-key-here"

# Oder in .env Datei
echo "OPENAI_API_KEY=your-api-key-here" > .env
```

### ğŸ›ï¸ Crawling-Parameter

| Parameter | Standard | Beschreibung |
|-----------|----------|-------------|
| **Max. Tiefe** | 3 | Anzahl Unterseiten pro Organisation |
| **Retry-Versuche** | 3 | Wiederholungen bei Fehlern |
| **Timeout** | 30s | Maximale Wartezeit pro Seite |
| **Konfidenz-Schwellwert** | 0.7 | Mindestvertrauen fÃ¼r Bewertungen |
| **ParallelitÃ¤t** | 5 | Gleichzeitige Crawling-Prozesse |

### ğŸ“Š Visualisierungs-Einstellungen

```python
# In settings_page.py anpassbar
VISUALIZATION_CONFIG = {
    'dpi': 300,  # AuflÃ¶sung der PNG-Dateien
    'figsize': (12, 8),  # DiagrammgrÃ¶ÃŸe
    'style': 'seaborn-v0_8',  # Matplotlib-Stil
    'color_palette': 'viridis'  # Farbschema
}
```

## ğŸš¨ Troubleshooting

### âŒ HÃ¤ufige Probleme

#### **OpenAI API-Fehler**
```
Error: OpenAI API key not found
```
**LÃ¶sung:** API-SchlÃ¼ssel in den Einstellungen konfigurieren

#### **Crawling-Timeouts**
```
Timeout error after 30 seconds
```
**LÃ¶sung:** Timeout-Werte in den erweiterten Einstellungen erhÃ¶hen

#### **Speicher-Probleme**
```
MemoryError: Unable to allocate array
```
**LÃ¶sung:** Anzahl paralleler Prozesse reduzieren oder Organisationen in kleineren Batches verarbeiten

#### **Visualisierungs-Fehler**
```
ModuleNotFoundError: No module named 'matplotlib'
```
**LÃ¶sung:** 
```bash
pip install matplotlib seaborn
```

### ğŸ” Debug-Modus

```bash
# Detaillierte Logs aktivieren
streamlit run app.py --logger.level=DEBUG

# Einzelne Komponenten testen
python -c "from src.crawler.crawl4ai_crawler import *; test_connection()"
```

### ğŸ“‹ System-Anforderungen

| Komponente | Minimum | Empfohlen |
|------------|---------|----------|
| **Python** | 3.8 | 3.11+ |
| **RAM** | 4GB | 8GB+ |
| **Speicher** | 2GB | 10GB+ |
| **CPU** | 2 Kerne | 4+ Kerne |
| **Internet** | Stabil | Hochgeschwindigkeit |

## ğŸ”„ Updates & Migration

### ğŸ“¦ AbhÃ¤ngigkeiten aktualisieren

```bash
# Alle Pakete aktualisieren
pip install --upgrade -r requirements.txt

# Spezifische Pakete
pip install --upgrade streamlit crawl4ai openai
```

### ğŸ—‚ï¸ Daten-Migration

Bei Updates werden bestehende CSV-Dateien und Visualisierungen automatisch beibehalten. Neue Features sind rÃ¼ckwÃ¤rtskompatibel.

## ğŸ¤ Beitragen

### ğŸš€ Entwicklung

1. **Fork** des Repositories
2. **Feature-Branch** erstellen (`git checkout -b feature/AmazingFeature`)
3. **Ã„nderungen** committen (`git commit -m 'Add AmazingFeature'`)
4. **Branch** pushen (`git push origin feature/AmazingFeature`)
5. **Pull Request** erstellen

### ğŸ“‹ Beitrag-Richtlinien

- **Code-Stil:** Folgen Sie PEP 8 und verwenden Sie `ruff` fÃ¼r Formatierung
- **Tests:** FÃ¼gen Sie Tests fÃ¼r neue Features hinzu
- **Dokumentation:** Aktualisieren Sie README und Docstrings
- **Commits:** Verwenden Sie aussagekrÃ¤ftige Commit-Nachrichten

### ğŸ› Bug Reports

Beim Melden von Bugs bitte folgende Informationen angeben:
- Python-Version
- Betriebssystem
- VollstÃ¤ndige Fehlermeldung
- Schritte zur Reproduktion
- Screenshots (falls relevant)

## ğŸ“„ Lizenz

Dieses Projekt steht unter der **Apache 2.0 Lizenz**. Siehe `LICENSE` Datei fÃ¼r Details.

## ğŸ†˜ Support

### ğŸ“ Kontakt

- **Issues:** [GitHub Issues](https://github.com/janschachtschabel/offenheitscrawler/issues)
- **Diskussionen:** [GitHub Discussions](https://github.com/janschachtschabel/offenheitscrawler/discussions)
- **E-Mail:** support@offenheitscrawler.de

### ğŸ“š Weitere Ressourcen

- **Dokumentation:** [docs/](docs/)
- **Beispiele:** [data/examples/](data/examples/)
- **Video-Tutorials:** [YouTube Playlist](https://youtube.com/playlist)
- **API-Referenz:** [docs/api/](docs/api/)

---

**Entwickelt mit â¤ï¸ fÃ¼r mehr Transparenz und Offenheit**
