#!/usr/bin/env python3
"""
Test script for LLM integration in Offenheitscrawler.
"""

import os
import asyncio
from src.llm.llm_client import LLMClient, LLMConfig


async def test_llm_integration():
    """Test the LLM integration functionality."""
    print("ğŸ§ª Testing LLM Integration for Offenheitscrawler")
    print("=" * 50)
    
    # Check for API key
    api_key = os.getenv('OPENAI_API_KEY')
    if not api_key:
        print("âŒ No OPENAI_API_KEY environment variable found.")
        print("ğŸ’¡ Please set your OpenAI API key:")
        print("   export OPENAI_API_KEY='your-api-key-here'")
        return
    
    print(f"âœ… Found API key: {api_key[:8]}...")
    
    # Create LLM client
    config = LLMConfig(
        api_key=api_key,
        model="gpt-4o-mini",
        temperature=0.3,
        max_tokens=2048
    )
    
    client = LLMClient(config)
    print(f"ğŸ¤– Created LLM client with model: {config.model}")
    
    # Test connection
    print("\nğŸ”— Testing connection...")
    try:
        success = await client.test_connection()
        if success:
            print("âœ… Connection successful!")
        else:
            print("âŒ Connection failed!")
            return
    except Exception as e:
        print(f"âŒ Connection error: {str(e)}")
        return
    
    # Test content analysis
    print("\nğŸ“ Testing content analysis...")
    
    sample_content = """
    Unsere UniversitÃ¤t ist stolz darauf, Transparenz und Offenheit zu fÃ¶rdern. 
    Wir verÃ¶ffentlichen regelmÃ¤ÃŸig unsere Forschungsergebnisse und stellen 
    Open Access Publikationen zur VerfÃ¼gung. Unsere Bibliothek bietet freien 
    Zugang zu wissenschaftlichen Ressourcen. DarÃ¼ber hinaus engagieren wir uns 
    fÃ¼r Open Science Initiativen und unterstÃ¼tzen die Prinzipien von Open Data.
    """
    
    criterion_name = "Open Access Publikationen"
    criterion_description = "Die Organisation stellt wissenschaftliche Publikationen frei zugÃ¤nglich zur VerfÃ¼gung"
    patterns = ["open access", "freier zugang", "open science", "publikationen"]
    
    try:
        analysis = await client.analyze_content_for_criteria(
            content=sample_content,
            criterion_name=criterion_name,
            criterion_description=criterion_description,
            patterns=patterns
        )
        
        print(f"ğŸ“Š Analysis Results:")
        print(f"   Fulfilled: {analysis.get('fulfilled', False)}")
        print(f"   Confidence: {analysis.get('confidence', 0.0):.2f}")
        print(f"   Justification: {analysis.get('justification', 'N/A')}")
        print(f"   Evidence: {analysis.get('evidence', [])}")
        print(f"   Found Patterns: {analysis.get('found_patterns', [])}")
        
    except Exception as e:
        print(f"âŒ Content analysis failed: {str(e)}")
        return
    
    # Test enhanced pattern matching
    print("\nğŸ” Testing enhanced pattern matching...")
    
    try:
        enhanced_result = await client.enhance_pattern_matching(
            content=sample_content,
            patterns=patterns,
            context="UniversitÃ¤re Offenheit und Transparenz"
        )
        
        print(f"ğŸ¯ Enhanced Pattern Results:")
        print(f"   Matches Found: {enhanced_result.get('matches_found', False)}")
        print(f"   Confidence: {enhanced_result.get('confidence', 0.0):.2f}")
        print(f"   Semantic Matches: {enhanced_result.get('semantic_matches', [])}")
        print(f"   Explanation: {enhanced_result.get('explanation', 'N/A')}")
        
    except Exception as e:
        print(f"âŒ Enhanced pattern matching failed: {str(e)}")
        return
    
    # Test summarization
    print("\nğŸ“‹ Testing summarization...")
    
    sample_evaluation = {
        "organization_name": "Test UniversitÃ¤t",
        "total_criteria": 10,
        "fulfilled_criteria": 7,
        "fulfillment_percentage": 70.0,
        "confidence_score": 0.85,
        "details": "Good performance in transparency and open access"
    }
    
    try:
        summary = await client.summarize_organization_analysis(
            organization_name="Test UniversitÃ¤t",
            evaluation_results=sample_evaluation
        )
        
        print(f"ğŸ“„ Generated Summary:")
        print(f"   {summary}")
        
    except Exception as e:
        print(f"âŒ Summarization failed: {str(e)}")
        return
    
    print("\nğŸ‰ All tests completed successfully!")
    print("âœ… LLM integration is working properly.")


if __name__ == "__main__":
    asyncio.run(test_llm_integration())
